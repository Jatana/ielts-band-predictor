# configs/train.yaml
# ------------------------------------------------------------
# Main experiment config for scripts/train.py (Hydra 1.3+)
# ------------------------------------------------------------

defaults:
  # pick *one* variant from each group ↓
  - datamodule: default # -> configs/datamodule/default.yaml
  - model: bert_reg # -> configs/model/bert_reg.yaml
  - optimizer: adamw # -> configs/optimizer/adamw.yaml

  # a list-group: you can enable/disable callbacks independently
  - callback_early_stop: early_stop
  - callback_lr_monitor: lr_monitor
  - logger: mlflow

  # keep this last – local overrides win
  - _self_

# -----------------------------------------------------------------
# Global experiment settings
# -----------------------------------------------------------------
seed: 42 # Lightning + numpy + torch seed

# Everything under `trainer:` is passed verbatim to pl.Trainer(**…)
trainer:
  accelerator: gpu # "cpu", "mps" or "gpu"
  devices: 1 # 1 (= first GPU) or a list [0,1] …
  max_epochs: 5
  precision: 16 # mixed precision for speed on Ampere+
  log_every_n_steps: 20
  num_sanity_val_steps: 2 # quick sanity check
  deterministic: true # ensure reproducibility
  # strategy: ddp            # uncomment if you need multi-GPU DDP

# Optional: Hydra runtime behaviour
hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S} # where checkpoints & .hydra go
  sweep:
    dir: multiruns/${now:%Y-%m-%d_%H-%M-%S}
    subdir: ${hydra.job.num}
